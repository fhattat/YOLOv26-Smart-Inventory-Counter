{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW2gWTG0fZeo"
      },
      "source": [
        "# ðŸš€  Project: Smart Inventory & Fruit Counting System (YOLOv26)\n",
        "\n",
        "**Scenario**:\n",
        "\n",
        "Detecting and counting apples moving on a conveyor belt using a vertical virtual line. Method: Custom Coordinate-Based Counting (Manual Buffer Logic).\n",
        "\n",
        "NOTE: I recommend running this project notebook with Google Colab using a T4 GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXwLRNmgosOJ"
      },
      "source": [
        "## ðŸŽ Smart Inventory & Object Tracking System (YOLOv26)\n",
        "\n",
        "ðŸ“‹ Project Overview\n",
        "\n",
        "This project demonstrates a real-time Computer Vision solution designed for automated inventory management in industrial environments. Using the state-of-the-art YOLOv26 object detection architecture, the system identifies, tracks, and counts items (e.g., fruits) moving on a conveyor belt with high precision.Unlike standard counting methods that rely on simple line-crossing algorithms, this project implements a Custom Buffer Zone Logic to ensure 100% accuracy. This approach mitigates common issues such as double-counting caused by video jitter or object occlusion.\n",
        "\n",
        "ðŸŽ¯ Key Features\n",
        "\n",
        "Object Detection: Fine-tuned YOLOv26 model to detect specific objects (e.g., Apples) from a custom dataset.Persistent Tracking: Utilizes advanced tracking algorithms (BoT-SORT/ByteTrack) to assign unique IDs to moving objects.\n",
        "\n",
        "Robust Counting Logic: Implements a \"Safety Corridor\" (Buffer Zone) mechanism. Objects are counted only when their centroid coordinates stabilize within a specific vertical range ($X \\pm Offset$).\n",
        "\n",
        "Real-Time Dashboard: Displays live analytics including bounding boxes, tracking IDs, and total count directly on the video feed.\n",
        "\n",
        "ðŸ› ï¸ Tech Stack & ToolsDeep\n",
        "\n",
        "Learning: Ultralytics YOLOv26 (Transfer Learning)\n",
        "\n",
        "Computer Vision: OpenCV (cv2) for frame processing and visualization\n",
        "\n",
        "Data Management: Roboflow API (Automated Dataset Ingestion)\n",
        "\n",
        "Infrastructure: Google Colab (GPU Acceleration)\n",
        "\n",
        "Project Link: https://github.com/fhattat/YOLOv26-Smart-Inventory-Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygEd35W4fvBo"
      },
      "source": [
        "## STEP 1: Environment Setup & Library Installation\n",
        "\n",
        "We need to install the ultralytics library for YOLOv26 models and roboflow to fetch the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S2XrG7ifCrY",
        "outputId": "b28d85fc-ddab-4a00-8449-be4a11fefa96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.4.9-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.13-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.13.0.90)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2026.1.4)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.5-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.2.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.4.9-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading roboflow-1.2.13-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.2.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.5-cp312-cp312-manylinux_2_28_x86_64.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m126.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, ultralytics-thop, roboflow, ultralytics\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.13.0.90\n",
            "    Uninstalling opencv-python-headless-4.13.0.90:\n",
            "      Successfully uninstalled opencv-python-headless-4.13.0.90\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.2.0 pillow-avif-plugin-1.5.5 roboflow-1.2.13 ultralytics-8.4.9 ultralytics-thop-2.0.18\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.12/dist-packages (from opencv-python-headless) (2.0.2)\n",
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: Install Dependencies\n",
        "# We use the '!' command to run terminal commands in Colab.\n",
        "\n",
        "!pip install ultralytics roboflow\n",
        "!pip install opencv-python-headless  # OpenCV for video processing\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import os\n",
        "import numpy as np\n",
        "from IPython.display import display, Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUE3Oy_MfCuk",
        "outputId": "fca2de4a-4816-4f0d-efb7-1b10a4b17673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Setup Completed.\n",
            "ðŸ”¥ GPU Available: True\n",
            "Thu Jan 29 20:47:34 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8             11W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability for faster training\n",
        "print(f\"âœ… Setup Completed.\")\n",
        "print(f\"ðŸ”¥ GPU Available: {torch.cuda.is_available()}\")\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9e4-CUdgIOC"
      },
      "source": [
        "## STEP 2: Download Dataset (Roboflow)\n",
        "\n",
        "We are pulling the \"Fruit Detection\" dataset directly from Roboflow Universe using your API key.\n",
        "\n",
        "Link for the data set used:\n",
        "\n",
        "https://universe.roboflow.com/toronto-metropolitan-university/fruit-detection-vibck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51nx8WOqfCxk",
        "outputId": "34db3e8f-ce07-417d-8df6-fb9db83a5668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in Fruit-Detection-4 to yolo26:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292631/292631 [00:04<00:00, 67356.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Fruit-Detection-4 in yolo26:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13376/13376 [00:01<00:00, 6797.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Data set path: /content/Fruit-Detection-4/data.yaml\n"
          ]
        }
      ],
      "source": [
        "# STEP 2: Load Dataset from Roboflow\n",
        "from roboflow import Roboflow\n",
        "\n",
        "try:\n",
        "    rf = Roboflow(api_key=\"o5YVlzSvoMb7P7brUGmp\")   # API_KEY_GOES_HERE FROM ROBOFLOW\n",
        "    project = rf.workspace(\"toronto-metropolitan-university\").project(\"fruit-detection-vibck\")\n",
        "    version = project.version(4)\n",
        "    dataset = version.download(\"yolo26\")\n",
        "\n",
        "    dataset_yaml = dataset.location + \"/data.yaml\"\n",
        "    print(f\"âœ… Data set path: {dataset_yaml}\")\n",
        "except:\n",
        "    print(\"âš ï¸ API Key not entered. Training step will be skipped or demo will be performed.\")\n",
        "    dataset_yaml = None  # Defining the dataset path variable for later use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnCpHLdrhkwf"
      },
      "source": [
        "## STEP 3: Model Training (Transfer Learning)\n",
        "\n",
        "We will fine-tune the yolo26m (Medium) model on our fruit dataset.\n",
        "For high-level training, â€œyolo26x.ptâ€ can be selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5ZW7IDCfC0o",
        "outputId": "d7747071-f1bc-4549-d0c0-bdfacbcc0bef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m.pt to 'yolo26m.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 42.2MB 93.1MB/s 0.5s\n",
            "ðŸš€ Training Started... (This may take a while)\n",
            "Ultralytics 8.4.9 ðŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/Fruit-Detection-4/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo26m.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=fruit_counter_final, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/fruit_counter_final, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 29.7MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
            "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
            "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
            "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
            "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5, 3, True]        \n",
            " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n",
            " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
            " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1   1974784  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True, 0.5, True]\n",
            " 23        [16, 19, 22]  1   2810952  ultralytics.nn.modules.head.Detect           [8, 1, True, [256, 512, 512]] \n",
            "YOLO26m summary: 280 layers, 21,785,224 parameters, 21,785,224 gradients, 74.8 GFLOPs\n",
            "\n",
            "Transferred 756/768 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt to 'yolo26n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.3MB 92.5MB/s 0.1s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1356.8Â±456.3 MB/s, size: 45.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Fruit-Detection-4/train/labels... 4675 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 4675/4675 2.5Kit/s 1.9s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/Fruit-Detection-4/train/labels.cache\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 813, len(boxes) = 13611. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 672.9Â±527.2 MB/s, size: 47.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Fruit-Detection-4/valid/labels... 1331 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 1331/1331 1.1Kit/s 1.2s\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/Fruit-Detection-4/valid/images/3d3ddc3054b32eb7_jpg.rf.23dbaa724edbe24e17c4c0b60a14a028.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Fruit-Detection-4/valid/labels.cache\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 80, len(boxes) = 3898. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "Plotting labels to /content/runs/detect/fruit_counter_final/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 124 weight(decay=0.0), 136 weight(decay=0.0005), 136 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/fruit_counter_final\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/20      9.25G     0.9825      2.728    0.01248         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.4it/s 3:29\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 1.6it/s 25.6s\n",
            "                   all       1331       3898       0.52      0.393      0.428      0.322\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/20      9.56G      1.092      1.602    0.01415         17        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.5it/s 3:11\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 1.9it/s 21.6s\n",
            "                   all       1331       3898      0.568       0.49      0.488      0.353\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/20      9.31G      1.144      1.492    0.01497         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.5it/s 3:13\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.0it/s 20.7s\n",
            "                   all       1331       3898      0.605      0.566      0.581      0.412\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/20      9.29G      1.147      1.465    0.01553         12        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.6it/s 3:07\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.0it/s 20.7s\n",
            "                   all       1331       3898      0.713      0.599      0.641      0.463\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/20      9.38G      1.105      1.356    0.01453         17        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.6it/s 3:07\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.0it/s 20.9s\n",
            "                   all       1331       3898      0.636      0.637       0.66      0.481\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/20      9.29G      1.077      1.204    0.01416         19        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.6it/s 3:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.1it/s 20.5s\n",
            "                   all       1331       3898      0.691      0.639      0.668      0.485\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/20      9.42G       1.07      1.216    0.01385         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.6it/s 3:06\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.1it/s 20.4s\n",
            "                   all       1331       3898      0.741      0.656      0.732      0.542\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/20      9.39G      1.036      1.138    0.01337          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.6it/s 3:07\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.0it/s 21.4s\n",
            "                   all       1331       3898      0.747      0.696      0.735      0.543\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/20      9.41G      1.007      1.042    0.01298         17        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.5it/s 3:10\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.0it/s 20.7s\n",
            "                   all       1331       3898      0.749      0.729       0.76      0.566\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/20      9.48G     0.9772      1.024    0.01258         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.6it/s 3:07\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.0it/s 20.7s\n",
            "                   all       1331       3898      0.781      0.748      0.788      0.594\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/20      9.38G     0.9168     0.8588    0.01552         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.6it/s 3:07\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.0it/s 20.6s\n",
            "                   all       1331       3898      0.782      0.725      0.784      0.592\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/20      9.37G     0.8758     0.7969    0.01438          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.6it/s 3:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.1it/s 20.4s\n",
            "                   all       1331       3898      0.797      0.733       0.79        0.6\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/20       9.4G     0.8559     0.7424     0.0139          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.6it/s 3:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.0it/s 20.6s\n",
            "                   all       1331       3898      0.799      0.769      0.821      0.618\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/20      9.25G     0.8364      0.688    0.01338         10        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.6it/s 3:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.0it/s 20.6s\n",
            "                   all       1331       3898      0.793      0.764      0.828      0.636\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/20      9.38G     0.8059     0.6406    0.01279         16        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.6it/s 3:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.0it/s 20.7s\n",
            "                   all       1331       3898       0.82      0.784      0.836      0.639\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/20      9.48G      0.787     0.5701    0.01243          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.6it/s 3:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.0it/s 20.6s\n",
            "                   all       1331       3898      0.799      0.801      0.831      0.643\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/20      9.39G     0.7591     0.5422    0.01182          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.6it/s 3:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.0it/s 20.6s\n",
            "                   all       1331       3898      0.828      0.796      0.842       0.66\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/20      9.26G     0.7393     0.5148     0.0116          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.6it/s 3:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.1it/s 20.5s\n",
            "                   all       1331       3898      0.817      0.809      0.845      0.661\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/20      9.48G     0.7277     0.4866    0.01114          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.6it/s 3:05\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.1it/s 20.3s\n",
            "                   all       1331       3898      0.841      0.798       0.85      0.669\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/20      9.38G     0.7155     0.4792    0.01088          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 293/293 1.6it/s 3:04\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.1it/s 20.4s\n",
            "                   all       1331       3898       0.84      0.793      0.852      0.673\n",
            "\n",
            "20 epochs completed in 1.182 hours.\n",
            "Optimizer stripped from /content/runs/detect/fruit_counter_final/weights/last.pt, 44.0MB\n",
            "Optimizer stripped from /content/runs/detect/fruit_counter_final/weights/best.pt, 44.0MB\n",
            "\n",
            "Validating /content/runs/detect/fruit_counter_final/weights/best.pt...\n",
            "Ultralytics 8.4.9 ðŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO26m summary (fused): 132 layers, 20,355,620 parameters, 0 gradients, 67.9 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 42/42 2.1it/s 19.5s\n",
            "                   all       1331       3898      0.839      0.795      0.852      0.673\n",
            "                 Apple        352        986      0.889      0.849      0.913      0.753\n",
            "                Banana        181        567      0.784       0.48      0.589      0.442\n",
            "          Green Pepper         68        240      0.767      0.849      0.861        0.6\n",
            "                 Lemon        150        383      0.896       0.85      0.926      0.758\n",
            "                Orange        176        430       0.88      0.901      0.948      0.809\n",
            "            Red Pepper         83        234      0.741      0.756       0.78      0.556\n",
            "            Strawberry        303        756      0.915      0.947      0.979      0.887\n",
            "                Tomato         60        302      0.842      0.725      0.819       0.58\n",
            "Speed: 0.2ms preprocess, 10.5ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/fruit_counter_final\u001b[0m\n",
            "âœ… Training Completed Successfully.\n"
          ]
        }
      ],
      "source": [
        "# STEP 3: Train YOLO Model\n",
        "# We load the pre-trained YOLO26m model and train it on our new data.\n",
        "\n",
        "# Load a pre-trained model\n",
        "model = YOLO('yolo26m.pt')\n",
        "\n",
        "if dataset_yaml:\n",
        "    print(\"ðŸš€ Training Started... (This may take a while)\")\n",
        "\n",
        "    results = model.train(\n",
        "        data=dataset_yaml,  # Path to dataset config\n",
        "        epochs=20,          # Number of training cycles\n",
        "        imgsz=640,          # Image resolution\n",
        "        batch=16,           # Batch size\n",
        "        plots=True,         # Generate training graphs\n",
        "        name='fruit_counter_final' # Name of the project folder\n",
        "    )\n",
        "    print(\"âœ… Training Completed Successfully.\")\n",
        "\n",
        "    # Path to the best performing weights\n",
        "    best_model_path = '/content/runs/detect/fruit_counter_final/weights/best.pt'\n",
        "\n",
        "else:\n",
        "    print(\"âŒ Dataset not found. Cannot start training.\")\n",
        "    # Fallback to pre-trained model if training fails\n",
        "    best_model_path = 'yolo26m.pt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wATnHvuziksH"
      },
      "source": [
        "## STEP 4: Inference Configuration (Video Setup)\n",
        "\n",
        "Define the input video path and output settings. Note: Make sure to upload your test_video.mp4 to the Colab files section before running this. This test video was uploaded to GitHub Profile\n",
        "\n",
        "https://github.com/fhattat/YOLOv26-Smart-Inventory-Counter\n",
        "\n",
        "**NOTE**: This test video was created by Google Veo3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6znPF6HfC3n",
        "outputId": "82a41945-f722-45d9-fd8b-97f3e41c58da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Video Found: /content/test_video.mp4\n",
            "ðŸ§  Loading Model from: /content/runs/detect/fruit_counter_final/weights/best.pt\n"
          ]
        }
      ],
      "source": [
        "# STEP 4: Define Video Paths & Load Trained Model\n",
        "\n",
        "# Input video file (Upload this to Colab Files on the left)\n",
        "input_video_path = \"/content/test_video.mp4\"   # video must be uploaded to left pane\n",
        "output_video_path = \"/content/final_smart_counter.mp4\"\n",
        "\n",
        "# Check if video exists\n",
        "if not os.path.exists(input_video_path):\n",
        "    print(f\"âŒ Error: File '{input_video_path}' not found.\")\n",
        "    print(\"Please upload 'test_video.mp4' to the file section.\")\n",
        "else:\n",
        "    print(f\"âœ… Video Found: {input_video_path}\")\n",
        "\n",
        "# Load the custom trained model\n",
        "print(f\"ðŸ§  Loading Model from: {best_model_path}\")\n",
        "model = YOLO(best_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSsjbsaNkTn8"
      },
      "source": [
        "## STEP 5: The Core Logic (Manual Counting)\n",
        "\n",
        "This is the most critical part. We implement a Buffer Zone mechanism.\n",
        "\n",
        "**Vertical Line**: Placed in the center of the screen.\n",
        "\n",
        "**Offset**: A safe area (buffer) around the line.\n",
        "\n",
        "**Logic**: If (Line - Offset) < Object_Center < (Line + Offset), we count it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGJm69eNz2Db",
        "outputId": "d82bb6ce-4382-4993-818d-7f97632e1e4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš™ï¸ Video Resolution: 1920x1080\n",
            "ðŸ“ Counting Line at X=960 (Buffer: +/- 30px)\n",
            "ðŸŽ¬ Processing Video... Please wait.\n",
            "ðŸŽ Counted! ID: 789 | Total: 1\n",
            "ðŸŽ Counted! ID: 833 | Total: 2\n",
            "â³ Processed Frame 30 | Current Count: 2\n",
            "ðŸŽ Counted! ID: 958 | Total: 3\n",
            "ðŸŽ Counted! ID: 1107 | Total: 4\n",
            "â³ Processed Frame 60 | Current Count: 4\n",
            "ðŸŽ Counted! ID: 1075 | Total: 5\n",
            "ðŸŽ Counted! ID: 1036 | Total: 6\n",
            "ðŸŽ Counted! ID: 907 | Total: 7\n",
            "â³ Processed Frame 90 | Current Count: 7\n",
            "ðŸŽ Counted! ID: 1175 | Total: 8\n",
            "ðŸŽ Counted! ID: 1098 | Total: 9\n",
            "â³ Processed Frame 120 | Current Count: 9\n",
            "ðŸŽ Counted! ID: 1150 | Total: 10\n",
            "ðŸŽ Counted! ID: 1106 | Total: 11\n",
            "â³ Processed Frame 150 | Current Count: 11\n",
            "ðŸŽ Counted! ID: 1105 | Total: 12\n",
            "ðŸŽ Counted! ID: 1124 | Total: 13\n",
            "ðŸŽ Counted! ID: 1109 | Total: 14\n",
            "â³ Processed Frame 180 | Current Count: 14\n",
            "ðŸŽ Counted! ID: 1090 | Total: 15\n",
            "ðŸŽ Counted! ID: 1392 | Total: 16\n",
            "â³ Processed Frame 210 | Current Count: 16\n",
            "ðŸŽ Counted! ID: 1128 | Total: 17\n",
            "ðŸŽ Counted! ID: 1156 | Total: 18\n",
            "ðŸŽ Counted! ID: 1445 | Total: 19\n",
            "â³ Processed Frame 240 | Current Count: 19\n",
            "ðŸŽ Counted! ID: 1163 | Total: 20\n",
            "ðŸŽ Counted! ID: 1199 | Total: 21\n",
            "â³ Processed Frame 270 | Current Count: 21\n",
            "ðŸŽ Counted! ID: 1216 | Total: 22\n",
            "ðŸŽ Counted! ID: 1469 | Total: 23\n",
            "â³ Processed Frame 300 | Current Count: 23\n",
            "ðŸŽ Counted! ID: 1431 | Total: 24\n",
            "ðŸŽ Counted! ID: 1391 | Total: 25\n",
            "ðŸŽ Counted! ID: 1266 | Total: 26\n",
            "â³ Processed Frame 330 | Current Count: 26\n",
            "ðŸŽ Counted! ID: 1527 | Total: 27\n",
            "ðŸŽ Counted! ID: 1458 | Total: 28\n",
            "â³ Processed Frame 360 | Current Count: 28\n",
            "ðŸŽ Counted! ID: 1531 | Total: 29\n",
            "ðŸŽ Counted! ID: 1468 | Total: 30\n",
            "â³ Processed Frame 390 | Current Count: 30\n",
            "ðŸŽ Counted! ID: 1467 | Total: 31\n",
            "ðŸŽ Counted! ID: 1481 | Total: 32\n",
            "ðŸŽ Counted! ID: 1471 | Total: 33\n",
            "â³ Processed Frame 420 | Current Count: 33\n",
            "ðŸŽ Counted! ID: 1446 | Total: 34\n",
            "ðŸŽ Counted! ID: 1760 | Total: 35\n",
            "â³ Processed Frame 450 | Current Count: 35\n",
            "ðŸŽ Counted! ID: 1487 | Total: 36\n",
            "ðŸŽ Counted! ID: 1510 | Total: 37\n",
            "ðŸŽ Counted! ID: 1812 | Total: 38\n",
            "â³ Processed Frame 480 | Current Count: 38\n",
            "ðŸŽ Counted! ID: 1516 | Total: 39\n",
            "ðŸŽ Counted! ID: 1553 | Total: 40\n",
            "â³ Processed Frame 510 | Current Count: 40\n",
            "ðŸŽ Counted! ID: 1568 | Total: 41\n",
            "ðŸŽ Counted! ID: 1831 | Total: 42\n",
            "â³ Processed Frame 540 | Current Count: 42\n",
            "ðŸŽ Counted! ID: 1801 | Total: 43\n",
            "ðŸŽ Counted! ID: 1759 | Total: 44\n",
            "ðŸŽ Counted! ID: 1632 | Total: 45\n",
            "â³ Processed Frame 570 | Current Count: 45\n",
            "ðŸŽ Counted! ID: 1894 | Total: 46\n",
            "ðŸŽ Counted! ID: 1822 | Total: 47\n",
            "â³ Processed Frame 600 | Current Count: 47\n",
            "ðŸŽ Counted! ID: 1918 | Total: 48\n",
            "ðŸŽ Counted! ID: 1830 | Total: 49\n",
            "â³ Processed Frame 630 | Current Count: 49\n",
            "ðŸŽ Counted! ID: 1829 | Total: 50\n",
            "ðŸŽ Counted! ID: 1844 | Total: 51\n",
            "ðŸŽ Counted! ID: 1833 | Total: 52\n",
            "â³ Processed Frame 660 | Current Count: 52\n",
            "ðŸŽ Counted! ID: 1816 | Total: 53\n",
            "ðŸŽ Counted! ID: 2131 | Total: 54\n",
            "â³ Processed Frame 690 | Current Count: 54\n",
            "ðŸŽ Counted! ID: 1849 | Total: 55\n",
            "ðŸŽ Counted! ID: 1876 | Total: 56\n",
            "ðŸŽ Counted! ID: 2183 | Total: 57\n",
            "â³ Processed Frame 720 | Current Count: 57\n",
            "----------------------------------------\n",
            "âœ… SUCCESS! Video saved to: /content/final_smart_counter.mp4\n"
          ]
        }
      ],
      "source": [
        "# STEP 5: Process Video & Count Objects (Manual Logic)\n",
        "\n",
        "# Open Video Capture\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "\n",
        "# Get Video Properties\n",
        "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Initialize Video Writer to save output\n",
        "video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
        "\n",
        "# --- COUNTING CONFIGURATION ---\n",
        "center_x = w // 2        # X-Coordinate of the vertical line\n",
        "offset = 30              # Buffer zone in pixels (Sensitivity)\n",
        "total_count = 0          # Initialize counter\n",
        "counted_ids = []         # List to keep track of counted Object IDs\n",
        "\n",
        "print(f\"âš™ï¸ Video Resolution: {w}x{h}\")\n",
        "print(f\"ðŸ“ Counting Line at X={center_x} (Buffer: +/- {offset}px)\")\n",
        "print(\"ðŸŽ¬ Processing Video... Please wait.\")\n",
        "\n",
        "frame_count = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "    success, frame = cap.read()\n",
        "    if not success:\n",
        "        break # End of video\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "    # 1. Object Tracking\n",
        "    # conf=0.15: Filter out weak detections\n",
        "    # persist=True: Essential for tracking objects across frames\n",
        "    results = model.track(frame, persist=True, verbose=False, conf=0.15, iou=0.5)\n",
        "\n",
        "    # Get detection boxes and IDs\n",
        "    if results[0].boxes.id is not None:\n",
        "        boxes = results[0].boxes.xywh.cpu() # Get box coordinates (x, y, w, h)\n",
        "        track_ids = results[0].boxes.id.int().cpu().tolist() # Get unique tracking IDs\n",
        "\n",
        "        # Loop through each detected object\n",
        "        for box, track_id in zip(boxes, track_ids):\n",
        "            x, y, w_box, h_box = box\n",
        "\n",
        "            # Calculate Centroid (Center point of the object)\n",
        "            cx = int(x)\n",
        "            cy = int(y)\n",
        "\n",
        "            # --- COUNTING LOGIC (The \"Magic\" Part) ---\n",
        "            # Check if the object is inside the vertical buffer zone\n",
        "            if (center_x - offset) < cx < (center_x + offset):\n",
        "\n",
        "                # Check if this specific ID has been counted before\n",
        "                if track_id not in counted_ids:\n",
        "                    total_count += 1\n",
        "                    counted_ids.append(track_id) # Add to history\n",
        "\n",
        "                    # Visual Feedback: Flash a Green Line when counted\n",
        "                    cv2.line(frame, (center_x, 0), (center_x, h), (0, 255, 0), 4)\n",
        "                    print(f\"ðŸŽ Counted! ID: {track_id} | Total: {total_count}\")\n",
        "\n",
        "            # Draw a small red dot at the center of the object\n",
        "            cv2.circle(frame, (cx, cy), 5, (0, 0, 255), -1)\n",
        "\n",
        "    # 2. Visualization (Drawing on Frame)\n",
        "\n",
        "    # Draw Bounding Boxes (from Model)\n",
        "    frame = results[0].plot()\n",
        "\n",
        "    # Draw The Reference Lines (Blue)\n",
        "    # Main Center Line\n",
        "    cv2.line(frame, (center_x, 0), (center_x, h), (255, 0, 0), 2)\n",
        "    # Buffer Zone Limits (Thinner Cyan Lines)\n",
        "    cv2.line(frame, (center_x - offset, 0), (center_x - offset, h), (255, 255, 0), 1)\n",
        "    cv2.line(frame, (center_x + offset, 0), (center_x + offset, h), (255, 255, 0), 1)\n",
        "\n",
        "    # 3. Dashboard Display (Bottom Left)\n",
        "    text = f\"Total Count: {total_count}\"\n",
        "\n",
        "    # Create a background rectangle for better readability\n",
        "    (text_w, text_h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.5, 3)\n",
        "\n",
        "    # Rectangle Coordinates (Bottom Left)\n",
        "    rect_start = (20, h - 90)\n",
        "    rect_end = (40 + text_w, h - 30)\n",
        "\n",
        "    # Draw Black Background Rectangle\n",
        "    cv2.rectangle(frame, rect_start, rect_end, (0, 0, 0), -1)\n",
        "\n",
        "    # Draw Text (Yellow)\n",
        "    cv2.putText(frame, text, (30, h - 45), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 255), 3)\n",
        "\n",
        "    # Save Frame to Output Video\n",
        "    video_writer.write(frame)\n",
        "\n",
        "    # Progress Log\n",
        "    if frame_count % 30 == 0:\n",
        "        print(f\"â³ Processed Frame {frame_count} | Current Count: {total_count}\")\n",
        "\n",
        "# Release Resources\n",
        "cap.release()\n",
        "video_writer.release()\n",
        "# cv2.destroyAllWindows() <--- REMOVED TO PREVENT COLAB ERROR\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"âœ… SUCCESS! Video saved to: {output_video_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucGiF0C7khiF"
      },
      "source": [
        "## STEP 6: Download & Result Verification\n",
        "\n",
        "Since Colab cannot play videos directly with cv2.imshow, we provide a helper to download the result.\n",
        "\n",
        "If you use local Notebook (i.e. Anaconda Jupyter Notebook), you can use another show method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "J60c3jnGfC9h",
        "outputId": "730b286f-1d35-4bf3-8203-54e8846b193f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“‚ Preparing download for: /content/final_smart_counter.mp4\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_3b9e5e66-b48c-4467-9d37-219e0ec15057\", \"final_smart_counter.mp4\", 97655355)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Download started automatically.\n"
          ]
        }
      ],
      "source": [
        "# STEP 6: Download the Result Video\n",
        "from google.colab import files\n",
        "\n",
        "print(f\"ðŸ“‚ Preparing download for: {output_video_path}\")\n",
        "\n",
        "try:\n",
        "    files.download(output_video_path)\n",
        "    print(\"âœ… Download started automatically.\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Auto-download failed: {e}\")\n",
        "    print(\"ðŸ‘‰ Please download manually from the 'Files' sidebar on the left.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IXZQIwGfDAk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhPc5nZgfDDf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkZbeRIgfDGg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjIkI29BfDJg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1mGKw9OfDMi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfLmhQvcfDP3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prn1adf2fDS8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
